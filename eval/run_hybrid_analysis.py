"""
Phase 3c — Hybrid Tagging & Per-Category Threshold Analysis

Analyzes whether merging tags from multiple providers improves overall quality.
Also applies optimized per-category confidence thresholds from Phase 3a findings.

Usage:
    python run_hybrid_analysis.py
"""

import json
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

DATASETS_DIR = Path(__file__).parent / "datasets"
RESULTS_DIR = Path(__file__).parent / "results"

# ---------------------------------------------------------------------------
# Data loading
# ---------------------------------------------------------------------------


def load_ground_truth() -> list[dict]:
    path = DATASETS_DIR / "tagging_ground_truth.jsonl"
    entries = []
    with open(path) as f:
        for line in f:
            line = line.strip()
            if line:
                entries.append(json.loads(line))
    return entries


def load_alt_results() -> dict:
    path = RESULTS_DIR / "alt_tagging_results.json"
    with open(path) as f:
        return json.load(f)


def load_tagging_baseline() -> dict:
    path = RESULTS_DIR / "tagging_baseline.json"
    with open(path) as f:
        return json.load(f)


# ---------------------------------------------------------------------------
# Fuzzy matching (copied from run_alt_tagging.py for independence)
# ---------------------------------------------------------------------------

# Model-agnostic synonym map — generated by Claude from all ground truth tags
def _load_synonym_map() -> dict[str, set[str]]:
    synonyms_path = Path(__file__).parent / "datasets" / "tag_synonyms.json"
    if synonyms_path.exists():
        raw = json.loads(synonyms_path.read_text())
        result: dict[str, set[str]] = {}
        for tag, syns in raw.items():
            tag_lower = tag.lower().strip()
            all_variants = {tag_lower} | {s.lower().strip() for s in syns}
            result[tag_lower] = all_variants
        return result
    else:
        print("WARNING: datasets/tag_synonyms.json not found. Run generate_synonyms.py first.")
        return {}

SYNONYM_MAP = _load_synonym_map()

_SYNONYM_REVERSE: dict[str, str] = {}
for canonical, synonyms in SYNONYM_MAP.items():
    for syn in synonyms:
        if syn != canonical:
            _SYNONYM_REVERSE[syn] = canonical
for canonical in SYNONYM_MAP:
    _SYNONYM_REVERSE[canonical] = canonical


def fuzzy_match(tag_a: str, tag_b: str) -> bool:
    a = tag_a.lower().strip()
    b = tag_b.lower().strip()
    if a == b:
        return True
    if len(a) >= 3 and len(b) >= 3:
        if a in b or b in a:
            return True
    # Direct synonym set membership
    if a in SYNONYM_MAP and b in SYNONYM_MAP[a]:
        return True
    if b in SYNONYM_MAP and a in SYNONYM_MAP[b]:
        return True
    # Reverse canonical match
    canon_a = _SYNONYM_REVERSE.get(a)
    canon_b = _SYNONYM_REVERSE.get(b)
    if canon_a and canon_b and canon_a == canon_b:
        return True
    return False


def fuzzy_match_sets(provider_tags: set[str], expected_tags: set[str]) -> tuple[set[str], set[str]]:
    matched_provider = set()
    matched_expected = set()
    for ptag in provider_tags:
        for etag in expected_tags:
            if etag not in matched_expected and fuzzy_match(ptag, etag):
                matched_provider.add(ptag)
                matched_expected.add(etag)
                break
    return matched_provider, matched_expected


# ---------------------------------------------------------------------------
# Metrics
# ---------------------------------------------------------------------------


def compute_metrics(provider_tags: set[str], expected_tags: set[str], use_fuzzy: bool = True) -> dict:
    if not provider_tags and not expected_tags:
        return {"precision": 1.0, "recall": 1.0, "f1": 1.0}
    if not provider_tags:
        return {"precision": 1.0, "recall": 0.0, "f1": 0.0}
    if not expected_tags:
        return {"precision": 0.0, "recall": 1.0, "f1": 0.0}

    if use_fuzzy:
        matched_p, matched_e = fuzzy_match_sets(provider_tags, expected_tags)
        precision = len(matched_p) / len(provider_tags)
        recall = len(matched_e) / len(expected_tags)
    else:
        matched = provider_tags & expected_tags
        precision = len(matched) / len(provider_tags)
        recall = len(matched) / len(expected_tags)

    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0
    return {"precision": round(precision, 4), "recall": round(recall, 4), "f1": round(f1, 4)}


def mean_of(values: list[float]) -> float:
    return round(sum(values) / len(values), 4) if values else 0.0


# ---------------------------------------------------------------------------
# Hybrid strategies
# ---------------------------------------------------------------------------


def strategy_azure_only(entry: dict, alt_photo: dict | None) -> set[str]:
    """Baseline: just Azure tags."""
    return {t["tag"].lower() for t in entry["ai_tags"]}


def strategy_azure_plus_rek(entry: dict, alt_photo: dict | None) -> set[str]:
    """Azure tags + Rekognition tags (union, deduplicated via fuzzy)."""
    azure = {t["tag"].lower() for t in entry["ai_tags"]}
    if alt_photo and "rekognition" in alt_photo and "error" not in alt_photo["rekognition"].get("metrics", {}):
        rek = {t.lower() for t in alt_photo["rekognition"]["tags"]}
        return azure | rek
    return azure


def strategy_azure_plus_claude(entry: dict, alt_photo: dict | None) -> set[str]:
    """Azure tags + Claude tags (union)."""
    azure = {t["tag"].lower() for t in entry["ai_tags"]}
    if alt_photo and "claude" in alt_photo and "error" not in alt_photo["claude"].get("metrics", {}):
        claude = {t.lower() for t in alt_photo["claude"]["tags"]}
        return azure | claude
    return azure


def strategy_azure_plus_both(entry: dict, alt_photo: dict | None) -> set[str]:
    """Azure + Rekognition + Claude (all three)."""
    azure = {t["tag"].lower() for t in entry["ai_tags"]}
    if alt_photo:
        if "rekognition" in alt_photo and "error" not in alt_photo["rekognition"].get("metrics", {}):
            rek = {t.lower() for t in alt_photo["rekognition"]["tags"]}
            azure = azure | rek
        if "claude" in alt_photo and "error" not in alt_photo["claude"].get("metrics", {}):
            claude = {t.lower() for t in alt_photo["claude"]["tags"]}
            azure = azure | claude
    return azure


def strategy_azure_no_people(entry: dict, alt_photo: dict | None) -> set[str]:
    """Azure tags but exclude the 'people' category (which is systematically wrong)."""
    return {t["tag"].lower() for t in entry["ai_tags"] if t["category"] != "people"}


def strategy_azure_no_people_plus_rek(entry: dict, alt_photo: dict | None) -> set[str]:
    """Azure (no people) + Rekognition."""
    azure = {t["tag"].lower() for t in entry["ai_tags"] if t["category"] != "people"}
    if alt_photo and "rekognition" in alt_photo and "error" not in alt_photo["rekognition"].get("metrics", {}):
        rek = {t.lower() for t in alt_photo["rekognition"]["tags"]}
        return azure | rek
    return azure


def strategy_azure_optimized_threshold(entry: dict, alt_photo: dict | None) -> set[str]:
    """Azure tags with per-category optimized thresholds.

    From Phase 3a analysis:
    - tag category: threshold 0.0 (all are good, keep everything >= 0.5 as stored)
    - object category: threshold 0.0 (mixed quality but low count)
    - people category: REMOVE entirely (0% precision)
    """
    result = set()
    for t in entry["ai_tags"]:
        # Skip people category entirely
        if t["category"] == "people":
            continue
        # Keep manual tags always
        if t["category"] == "manual":
            result.add(t["tag"].lower())
            continue
        # Object tags: keep if confidence >= 0.6 (slightly higher threshold to reduce noise)
        if t["category"] == "object" and t["confidence"] >= 0.6:
            result.add(t["tag"].lower())
            continue
        # Tag category: keep all (already filtered at 0.5 by Azure)
        if t["category"] == "tag":
            result.add(t["tag"].lower())
            continue
        # Text category: keep all
        if t["category"] == "text":
            result.add(t["tag"].lower())
    return result


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------


def main():
    ground_truth = load_ground_truth()
    alt_results = load_alt_results()
    tagging_baseline = load_tagging_baseline()

    print(f"Loaded {len(ground_truth)} ground truth photos")

    # Build lookup: photoId -> alt result
    alt_by_photo = {p["photoId"]: p for p in alt_results["per_photo"]}

    # Define strategies
    strategies = {
        "Azure only (baseline)": strategy_azure_only,
        "Azure (no people)": strategy_azure_no_people,
        "Azure (optimized thresholds)": strategy_azure_optimized_threshold,
        "Azure + Rekognition": strategy_azure_plus_rek,
        "Azure + Claude Haiku": strategy_azure_plus_claude,
        "Azure + Rek + Claude": strategy_azure_plus_both,
        "Azure (no people) + Rek": strategy_azure_no_people_plus_rek,
    }

    # Evaluate each strategy
    results = {}
    for name, strategy_fn in strategies.items():
        photo_metrics = []
        for entry in ground_truth:
            photo_id = entry["photoId"]
            alt_photo = alt_by_photo.get(photo_id)

            # Get provider tags for this strategy
            provider_tags = strategy_fn(entry, alt_photo)

            # Expected tags = correct + missing
            expected = {t.lower() for t in entry.get("correct_tags", [])} | \
                       {t.lower() for t in entry.get("missing_tags", [])}

            metrics = compute_metrics(provider_tags, expected, use_fuzzy=True)
            photo_metrics.append(metrics)

        avg = {
            "precision": mean_of([m["precision"] for m in photo_metrics]),
            "recall": mean_of([m["recall"] for m in photo_metrics]),
            "f1": mean_of([m["f1"] for m in photo_metrics]),
        }
        results[name] = avg

    # Print comparison table
    print("\n" + "=" * 70)
    print("HYBRID TAGGING STRATEGY COMPARISON")
    print("=" * 70)
    print(f"\n{'Strategy':<35} {'Precision':>10} {'Recall':>10} {'F1':>10}")
    print("-" * 67)

    for name, metrics in results.items():
        marker = " <-- best" if metrics["f1"] == max(r["f1"] for r in results.values()) else ""
        print(f"{name:<35} {metrics['precision']:>10.3f} {metrics['recall']:>10.3f} {metrics['f1']:>10.3f}{marker}")

    # Analysis: what did removing people tags do?
    print("\n--- Impact Analysis ---")
    base = results["Azure only (baseline)"]
    no_people = results["Azure (no people)"]
    print(f"\nRemoving people tags:")
    print(f"  Precision: {base['precision']:.3f} -> {no_people['precision']:.3f} ({no_people['precision'] - base['precision']:+.3f})")
    print(f"  Recall:    {base['recall']:.3f} -> {no_people['recall']:.3f} ({no_people['recall'] - base['recall']:+.3f})")
    print(f"  F1:        {base['f1']:.3f} -> {no_people['f1']:.3f} ({no_people['f1'] - base['f1']:+.3f})")

    opt = results["Azure (optimized thresholds)"]
    print(f"\nOptimized thresholds (no people, object>=0.6):")
    print(f"  Precision: {base['precision']:.3f} -> {opt['precision']:.3f} ({opt['precision'] - base['precision']:+.3f})")
    print(f"  Recall:    {base['recall']:.3f} -> {opt['recall']:.3f} ({opt['recall'] - base['recall']:+.3f})")
    print(f"  F1:        {base['f1']:.3f} -> {opt['f1']:.3f} ({opt['f1'] - base['f1']:+.3f})")

    azure_rek = results["Azure + Rekognition"]
    print(f"\nAdding Rekognition tags:")
    print(f"  Precision: {base['precision']:.3f} -> {azure_rek['precision']:.3f} ({azure_rek['precision'] - base['precision']:+.3f})")
    print(f"  Recall:    {base['recall']:.3f} -> {azure_rek['recall']:.3f} ({azure_rek['recall'] - base['recall']:+.3f})")
    print(f"  F1:        {base['f1']:.3f} -> {azure_rek['f1']:.3f} ({azure_rek['f1'] - base['f1']:+.3f})")

    # Recommendations
    print("\n--- Recommendations ---")
    best_strategy = max(results.items(), key=lambda x: x[1]["f1"])
    print(f"\n1. Best strategy: '{best_strategy[0]}' (F1={best_strategy[1]['f1']:.3f})")

    if no_people["f1"] > base["f1"]:
        print("2. DISABLE people count tags in production — they are 100% incorrect")
        print("   Action: Set people category threshold to 1.0 (effectively disabled)")
    if azure_rek["recall"] > base["recall"] and azure_rek["f1"] > base["f1"]:
        print("3. CONSIDER adding Rekognition as secondary tagger — improves recall")
        print("   Trade-off: Added cost + latency vs. better photo discovery in chat")
    else:
        print("3. Adding Rekognition does NOT improve overall quality enough to justify cost")
        print("   The tag vocabulary mismatch and noise outweigh recall gains")

    print("\n4. Per-category confidence thresholds:")
    print("   - tag: keep current 0.5 (high precision, no change needed)")
    print("   - object: raise to 0.6 (reduces false positives like 'Teddy bear' on dog photos)")
    print("   - people: DISABLE (0% precision — systematic Azure CV failure)")
    print("   - text: keep current 0.5 (OCR is reliable)")
    print("   - manual: always keep (user-added, 100% precision by definition)")

    # Save results
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    output = {
        "metadata": {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "total_photos": len(ground_truth),
            "matching_mode": "fuzzy",
        },
        "strategies": results,
        "recommendations": {
            "best_strategy": best_strategy[0],
            "disable_people_tags": True,
            "object_threshold": 0.6,
            "tag_threshold": 0.5,
            "text_threshold": 0.5,
            "add_rekognition": False,
            "add_claude": False,
        },
    }

    output_path = RESULTS_DIR / "hybrid_analysis.json"
    with open(output_path, "w") as f:
        json.dump(output, f, indent=2)

    print(f"\nResults saved to {output_path}")


if __name__ == "__main__":
    main()
